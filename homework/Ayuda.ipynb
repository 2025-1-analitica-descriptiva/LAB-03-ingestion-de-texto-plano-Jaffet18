{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "127c137f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cluster', 'Cantidad de', 'Porcentaje de', 'Principales palabras clave']\n",
      "['palabras clave', 'palabras clave']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:44: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:44: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\jaffet.ramirez\\AppData\\Local\\Temp\\ipykernel_21316\\1319627320.py:44: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  sep='\\s{2,}',\n",
      "C:\\Users\\jaffet.ramirez\\AppData\\Local\\Temp\\ipykernel_21316\\1319627320.py:44: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  sep='\\s{2,}',\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Expected 4 fields in line 19, saw 6. Error could possibly be due to quotes being ignored when a multi-char delimiter is used.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 59\u001b[39m\n\u001b[32m     51\u001b[39m     df.columns = (df.columns\n\u001b[32m     52\u001b[39m                   .str.lower()\n\u001b[32m     53\u001b[39m                   .str.strip()\n\u001b[32m     54\u001b[39m                   .str.replace(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms+\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m, regex=\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m     56\u001b[39m     \u001b[38;5;28mprint\u001b[39m(df.columns.tolist())\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpregunta_01\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mpregunta_01\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     35\u001b[39m headers = [\n\u001b[32m     36\u001b[39m     header1[\u001b[32m0\u001b[39m],  \u001b[38;5;66;03m# Cluster\u001b[39;00m\n\u001b[32m     37\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheader1[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheader2[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m.strip(),  \u001b[38;5;66;03m# Cantidad de palabras clave\u001b[39;00m\n\u001b[32m     38\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheader1[\u001b[32m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheader2[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m.strip(),  \u001b[38;5;66;03m# Porcentaje de palabras clave\u001b[39;00m\n\u001b[32m     39\u001b[39m     header1[\u001b[32m3\u001b[39m]  \u001b[38;5;66;03m# Principales palabras clave\u001b[39;00m\n\u001b[32m     40\u001b[39m ]\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Leer los datos\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../files/input/clusters_report.txt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m                \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43ms\u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43m2,}\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m                \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpython\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m                \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m                \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m                \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Limpiar nombres de columnas\u001b[39;00m\n\u001b[32m     51\u001b[39m df.columns = (df.columns\n\u001b[32m     52\u001b[39m               .str.lower()\n\u001b[32m     53\u001b[39m               .str.strip()\n\u001b[32m     54\u001b[39m               .str.replace(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms+\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m, regex=\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub\\LABORATORIOS\\LAB-03-ingestion-de-texto-plano-Jaffet18\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub\\LABORATORIOS\\LAB-03-ingestion-de-texto-plano-Jaffet18\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub\\LABORATORIOS\\LAB-03-ingestion-de-texto-plano-Jaffet18\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub\\LABORATORIOS\\LAB-03-ingestion-de-texto-plano-Jaffet18\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:288\u001b[39m, in \u001b[36mPythonParser.read\u001b[39m\u001b[34m(self, rows)\u001b[39m\n\u001b[32m    285\u001b[39m     indexnamerow = content[\u001b[32m0\u001b[39m]\n\u001b[32m    286\u001b[39m     content = content[\u001b[32m1\u001b[39m:]\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m alldata = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rows_to_cols\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    289\u001b[39m data, columns = \u001b[38;5;28mself\u001b[39m._exclude_implicit_index(alldata)\n\u001b[32m    291\u001b[39m conv_data = \u001b[38;5;28mself\u001b[39m._convert_data(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub\\LABORATORIOS\\LAB-03-ingestion-de-texto-plano-Jaffet18\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:1063\u001b[39m, in \u001b[36mPythonParser._rows_to_cols\u001b[39m\u001b[34m(self, content)\u001b[39m\n\u001b[32m   1057\u001b[39m             reason = (\n\u001b[32m   1058\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mError could possibly be due to quotes being \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1059\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mignored when a multi-char delimiter is used.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1060\u001b[39m             )\n\u001b[32m   1061\u001b[39m             msg += \u001b[33m\"\u001b[39m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m + reason\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_alert_malformed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_num\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;66;03m# see gh-13320\u001b[39;00m\n\u001b[32m   1066\u001b[39m zipped_content = \u001b[38;5;28mlist\u001b[39m(lib.to_object_array(content, min_width=col_len).T)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub\\LABORATORIOS\\LAB-03-ingestion-de-texto-plano-Jaffet18\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:781\u001b[39m, in \u001b[36mPythonParser._alert_malformed\u001b[39m\u001b[34m(self, msg, row_num)\u001b[39m\n\u001b[32m    764\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    765\u001b[39m \u001b[33;03mAlert a user about a malformed row, depending on value of\u001b[39;00m\n\u001b[32m    766\u001b[39m \u001b[33;03m`self.on_bad_lines` enum.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    778\u001b[39m \u001b[33;03m    even though we 0-index internally.\u001b[39;00m\n\u001b[32m    779\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.on_bad_lines == \u001b[38;5;28mself\u001b[39m.BadLineHandleMethod.ERROR:\n\u001b[32m--> \u001b[39m\u001b[32m781\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ParserError(msg)\n\u001b[32m    782\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.on_bad_lines == \u001b[38;5;28mself\u001b[39m.BadLineHandleMethod.WARN:\n\u001b[32m    783\u001b[39m     warnings.warn(\n\u001b[32m    784\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSkipping line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    785\u001b[39m         ParserWarning,\n\u001b[32m    786\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    787\u001b[39m     )\n",
      "\u001b[31mParserError\u001b[39m: Expected 4 fields in line 19, saw 6. Error could possibly be due to quotes being ignored when a multi-char delimiter is used."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Escriba el codigo que ejecute la accion solicitada en cada pregunta.\n",
    "\"\"\"\n",
    "\n",
    "# pylint: disable=import-outside-toplevel\n",
    "\n",
    "\n",
    "def pregunta_01():\n",
    "    \"\"\"\n",
    "    Construya y retorne un dataframe de Pandas a partir del archivo\n",
    "    'files/input/clusters_report.txt'. Los requierimientos son los siguientes:\n",
    "\n",
    "    - El dataframe tiene la misma estructura que el archivo original.\n",
    "    - Los nombres de las columnas deben ser en minusculas, reemplazando los\n",
    "      espacios por guiones bajos.\n",
    "    - Las palabras clave deben estar separadas por coma y con un solo\n",
    "      espacio entre palabra y palabra.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import re\n",
    "\n",
    "    with open('../files/input/clusters_report.txt', 'r', encoding='utf-8') as f:\n",
    "        lines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "    # Extraer y combinar los encabezados\n",
    "    header1 = re.split(r'\\s{2,}', lines[0])\n",
    "    header2 = re.split(r'\\s{2,}', lines[1])\n",
    "    print(header1)\n",
    "    print(header2)\n",
    "\n",
    "    # Construir encabezados combinados\n",
    "    # Construir encabezados combinados\n",
    "    headers = [\n",
    "        header1[0],  # Cluster\n",
    "        f\"{header1[1]} {header2[0]}\".strip(),  # Cantidad de palabras clave\n",
    "        f\"{header1[2]} {header2[1]}\".strip(),  # Porcentaje de palabras clave\n",
    "        header1[3]  # Principales palabras clave\n",
    "    ]\n",
    "\n",
    "    # Leer los datos\n",
    "    df = pd.read_csv('../files/input/clusters_report.txt',\n",
    "                    sep='\\s{2,}',\n",
    "                    engine='python',\n",
    "                    skiprows=3,\n",
    "                    header=None,\n",
    "                    names=headers)\n",
    "\n",
    "    # Limpiar nombres de columnas\n",
    "    df.columns = (df.columns\n",
    "                  .str.lower()\n",
    "                  .str.strip()\n",
    "                  .str.replace(r'\\s+', '_', regex=True))\n",
    "\n",
    "    print(df.columns.tolist())\n",
    "\n",
    "\n",
    "print(pregunta_01())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506912a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['         palabras clave  palabras clave', '---------------------------------------------------------------------------------------------------------------------', '   1     105             15,9 %          maximum power point tracking, fuzzy-logic based control, photo voltaic (pv), ', '                                         photo-voltaic  system,  differential   evolution   algorithm,   evolutionary', '                                         algorithm, double-fed induction generator (dfig), ant  colony  optimisation, ', '                                         photo voltaic array, firefly algorithm, partial shade.', '   2     102             15,4 %          support vector machine,  long  short-term  memory,  back-propagation  neural', '                                         network,  convolution  neural  network,  speed   wind   prediction,   energy', '                                         consumption,   wind   power   forecasting,   extreme    learning    machine,', '                                         recurrent-neural-network (rnn), radial basis function (rbf)  networks,  wind ', '                                         farm.', '   3     89              13,4 %          smart grid, wind power, reinforcement learning,  energy  management,  energy ', '                                         efficiency, solar energy, deep reinforcement learning, demand-response (dr),', '                                         internet of things, energy harvester, q-learning.', '   4     60              9,1 %           wind   turbine,   fault    diagnosis,    biodiesel,    failure    detection,', '                                         response-surface methodology, condition monitoring, load forecasting, energy', '                                         consumption forecast,  anomalies  detection,  optimization-based  algorithm, ', '                                         supervisory control and data acquisition.', '   5     52              7,9 %           electric vehicle, lithium-ion batteries, state of charge, state  of  health,', '                                         hybrid-electric vehicle, energy  management  strategies,  energy  management ', '                                         system, remaining useful life, battery management system, transfer learning, ', '                                         hybrid energy storage.', '   6     51              7,7 %           particle  swarm  optimization,  distribute  generation,  solar   irradiance,', '                                         artificial  bee   colonies,   energy   storage   systems,   bat   algorithm,', '                                         gravitational search algorithm, distributed  system,  multi-objective  swarm', '                                         optimization  (mopso),  optimal  power-flow  (opf),  load-frequency  control', '   7     42              6,3 %           multi-objective   optimization,   energy   storage,    economic    dispatch,', '                                         non-dominated  sorting  genetic  algorithm,   sensitive   analysis,   hybrid', '                                         renewable energy source, plug-in electric vehicle, combined-heat and  power,', '                                         multi-objective genetic algorithm, unit-commitment, dc-dc converters.', '   8     38              5,7 %           genetic algorithm, demand-side  management,  energy-saving,  hybrid  electric ', '                                         system (hes), wind turbine blade, data-driven modelling, simulated annealing, ', '                                         solar forecasting, geographic information system,  renewable  energy  system,', '                                         cogeneration.', '   9     35              5,3 %           anfis,   global   solar   irradiance,  solar  irradiance  forecast,   genetic', '                                         programing,   islanding   detection,   expert  system,  distributed networks, ', '                                         evolutionary computation,  wavelet-based  neural  network,   root mean square ', '                                         errors, virtual power plant.', '   10    27              4,1 %           micro grid,  multi-agent  systems,  distributed  energy  resource,  batteries ', '                                         energy storage system, dc micro grid, pitch-control,  droop  control,  hybrid ', '                                         ac/dc microgrid, voltage regulation, superconducting magnetic energy storage, ', '                                         distributed-control.', '   11    22              3,3 %           hydrogen,  biochar,  biomass,  biogas,  microbial  fuel  cell,  gasification, ', '                                         biofuel, ethanol, engine performance, pyrolysis, anaerobic digester.', '   12    22              3,3 %           state of charge (soc) estimation,  radial  basis  function,  short-term  load', '                                         forecasting,  computational  fluid  dynamics,  generalized-regression  neural', '                                         network,  monte-carlo   simulation,   multiple   linear   regression,   power', '                                         generation,   nonlinear   auto-regressive   exogenous   (narx)  model  neural', '                                         networks, surrogate model, extreme gradient boosting.', '   13    17              2,6 %           pem   fuel   cell,   solid-oxide  fuel  cell,  deep-belief  networks,  energy', '                                         optimisation, particles-size  distributions,  biomass  gasification,  exergy, ', '                                         battery    management,    hydrogen     production,     numeric    simulation, ', '                                         system-identification.']\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Escriba el codigo que ejecute la accion solicitada en cada pregunta.\n",
    "\"\"\"\n",
    "\n",
    "# pylint: disable=import-outside-toplevel\n",
    "\n",
    "\n",
    "def pregunta_01():\n",
    "    \"\"\"\n",
    "    Construya y retorne un dataframe de Pandas a partir del archivo\n",
    "    'files/input/clusters_report.txt'. Los requierimientos son los siguientes:\n",
    "\n",
    "    - El dataframe tiene la misma estructura que el archivo original.\n",
    "    - Los nombres de las columnas deben ser en minusculas, reemplazando los\n",
    "      espacios por guiones bajos.\n",
    "    - Las palabras clave deben estar separadas por coma y con un solo\n",
    "      espacio entre palabra y palabra.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import re\n",
    "\n",
    "    with open('../files/input/clusters_report.txt', 'r', encoding='utf-8') as f:\n",
    "        lines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "        line_5 = lines[7].strip()\n",
    "        #print(line_5[0])\n",
    "    # Eliminar líneas vacías\n",
    "    # Extraer y combinar los encabezados\n",
    "    header1 = re.split(r'\\s{2,}', lines[0])\n",
    "    header2 = re.split(r'\\s{2,}', lines[1])\n",
    "    #print(header1)\n",
    "    #print(header2)\n",
    "\n",
    "    # Construir encabezados combinados\n",
    "    headers = [\n",
    "        header1[0],  # Cluster\n",
    "        f\"{header1[1]} {header2[0]}\".strip(),  # Cantidad de palabras clave\n",
    "        f\"{header1[2]} {header2[1]}\".strip(),  # Porcentaje de palabras clave\n",
    "        header1[3]  # Principales palabras clave\n",
    "    ]\n",
    "    #print(headers)\n",
    "\n",
    "    df = pd.read_csv('../files/input/clusters_report.txt', sep='\\t')\n",
    "    df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "    columna_uno = df.iloc[:, 0].tolist() \n",
    "    \n",
    "    \n",
    "    return print(columna_uno)\n",
    "    #print(df.columns.tolist())\n",
    "    #return print(df.head())\n",
    "\n",
    "print(pregunta_01())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9f9fd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cluster  cantidad_de_palabras_clave  porcentaje_de_palabras_clave  \\\n",
      "0        1                         105                          15.9   \n",
      "1        2                         102                          15.4   \n",
      "2        3                          89                          13.4   \n",
      "3        4                          60                           9.1   \n",
      "4        5                          52                           7.9   \n",
      "\n",
      "                          principales_palabras_clave  \n",
      "0  maximum power point tracking, fuzzy-logic base...  \n",
      "1  support vector machine, long short-term memory...  \n",
      "2  smart grid, wind power, reinforcement learning...  \n",
      "3  wind turbine, fault diagnosis, biodiesel, fail...  \n",
      "4  electric vehicle, lithium-ion batteries, state...  \n",
      "maximum power point tracking, fuzzy-logic based control, photo voltaic (pv), photo-voltaic system, differential evolution algorithm, evolutionary algorithm, double-fed induction generator (dfig), ant colony optimisation, photo voltaic array, firefly algorithm, partial shade.\n",
      "Respuesta incorrecta\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Leer el archivo como texto plano\n",
    "with open('../files/input/clusters_report.txt', 'r', encoding='utf-8') as file:\n",
    "    #lines = file.readlines()\n",
    "    lines = [line.strip() for line in file.readlines() if line.strip()]\n",
    "\n",
    "# Construcción de encabezados\n",
    "header1 = re.split(r'\\s{2,}', lines[0])\n",
    "header2 = re.split(r'\\s{2,}', lines[1])\n",
    "\n",
    "# Construir encabezados combinados y separados por guiones bajos\n",
    "headers = [\n",
    "    header1[0],  # Cluster\n",
    "    f\"{header1[1]} {header2[0]}\".strip(),  # Cantidad de palabras clave\n",
    "    f\"{header1[2]} {header2[1]}\".strip(),  # Porcentaje de palabras clave\n",
    "    header1[3]  # Principales palabras clave\n",
    "]\n",
    "# Limpiar nombres de columnas\n",
    "headers = [header.lower().replace(' ', '_') for header in headers]\n",
    "\n",
    "# Procesar las líneas para extraer datos\n",
    "data = []\n",
    "current_cluster = None\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue  # Ignorar líneas vacías\n",
    "    \n",
    "    # Detectar líneas de cluster (ej: \"1     105             15,9 % ...\")\n",
    "    if re.match(r'^\\s*\\d+\\s+\\d+', line):\n",
    "        parts = re.split(r'\\s{2,}', line, maxsplit=3)  # Dividir en 4 partes\n",
    "        if len(parts) >= 4:\n",
    "            current_cluster = {\n",
    "                headers[0]: parts[0].strip(),\n",
    "                headers[1]: parts[1].strip(),\n",
    "                headers[2]: parts[2].strip(),\n",
    "                headers[3]: parts[3].strip()\n",
    "            }\n",
    "            data.append(current_cluster)\n",
    "    elif current_cluster:  # Continuación de palabras clave\n",
    "        current_cluster[headers[3]] += ' ' + line.strip()\n",
    "\n",
    "# Crear DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Limpiar espacios adicionales en palabras clave\n",
    "df[headers[3]] = df[headers[3]].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "# Convertir a enteros los datos de las columnas 0 y 1\n",
    "df[headers[0]] = df[headers[0]].astype(int)\n",
    "df[headers[1]] = df[headers[1]].str.replace(',', '').astype(int)\n",
    "df[headers[2]] = df[headers[2]].str.replace('%', '').str.replace(',', '.').astype(float)\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "print(df.head())\n",
    "\n",
    "print(df.principales_palabras_clave.to_list()[0])\n",
    "\n",
    "# Limpiar espacios adicionales en palabras clave\n",
    "df[headers[3]] = df[headers[3]].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "if(df.principales_palabras_clave.to_list()[0] == (\n",
    "        \"maximum power point tracking, \"\n",
    "        \"fuzzy-logic based control, \"\n",
    "        \"photo voltaic (pv), \"\n",
    "        \"photo-voltaic system, \"\n",
    "        \"differential evolution algorithm, \"\n",
    "        \"evolutionary algorithm, \"\n",
    "        \"double-fed induction generator (dfig), \"\n",
    "        \"ant colony optimisation, \"\n",
    "        \"photo voltaic array, \"\n",
    "        \"firefly algorithm, \"\n",
    "        \"partial shade\"\n",
    "    )):\n",
    "    print(\"Respuesta correcta\")\n",
    "else:\n",
    "    print(\"Respuesta incorrecta\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "477e6a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cluster  cantidad_de_palabras_clave  porcentaje_de_palabras_clave  \\\n",
      "0        1                         105                          15.9   \n",
      "1        2                         102                          15.4   \n",
      "2        3                          89                          13.4   \n",
      "3        4                          60                           9.1   \n",
      "4        5                          52                           7.9   \n",
      "\n",
      "                          principales_palabras_clave  \n",
      "0  maximum power point tracking, fuzzy-logic base...  \n",
      "1  support vector machine, long short-term memory...  \n",
      "2  smart grid, wind power, reinforcement learning...  \n",
      "3  wind turbine, fault diagnosis, biodiesel, fail...  \n",
      "4  electric vehicle, lithium-ion batteries, state...  \n",
      "maximum power point tracking, fuzzy-logic based control, photo voltaic (pv), photo-voltaic system, differential evolution algorithm, evolutionary algorithm, double-fed induction generator (dfig), ant colony optimisation, photo voltaic array, firefly algorithm, partial shade.\n",
      "Respuesta incorrecta\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def pregunta_01():\n",
    "    # Leer el archivo línea por línea\n",
    "    with open('files/input/clusters_report.txt', 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # Eliminar líneas vacías y de separación\n",
    "    lines = [line.rstrip() for line in lines if line.strip() and not line.startswith('---')]\n",
    "    \n",
    "    # Procesar los datos\n",
    "    data = []\n",
    "    current_cluster = None\n",
    "    \n",
    "    for line in lines:\n",
    "        # Saltar las líneas de encabezado\n",
    "        if line.startswith('Cluster') or line.startswith('Cantidad'):\n",
    "            continue\n",
    "        \n",
    "        # Verificar si es una nueva entrada de cluster\n",
    "        if re.match(r'^\\s*\\d+\\s+\\d+', line):\n",
    "            if current_cluster:  # Guardar el cluster anterior\n",
    "                data.append(current_cluster)\n",
    "            \n",
    "            # Procesar nueva línea de cluster\n",
    "            parts = re.split(r'\\s{2,}', line.strip(), maxsplit=3)\n",
    "            current_cluster = {\n",
    "                'cluster': int(parts[0]),\n",
    "                'cantidad_de_palabras_clave': int(parts[1]),\n",
    "                'porcentaje_de_palabras_clave': float(parts[2].replace(',', '.').replace('%', '').strip()),\n",
    "                'principales_palabras_clave': parts[3] if len(parts) > 3 else ''\n",
    "            }\n",
    "        elif current_cluster:  # Continuación de palabras clave\n",
    "            current_cluster['principales_palabras_clave'] += ' ' + line.strip()\n",
    "    \n",
    "    # Añadir el último cluster\n",
    "    if current_cluster:\n",
    "        data.append(current_cluster)\n",
    "    \n",
    "    # Crear DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Función para limpiar palabras clave exactamente como en el test\n",
    "    def clean_keywords(keywords):\n",
    "        # Normalizar espacios\n",
    "        cleaned = ' '.join(keywords.split())\n",
    "        # Manejar comas específicamente\n",
    "        cleaned = re.sub(r',\\s*', ', ', cleaned)\n",
    "        # Eliminar punto final si existe\n",
    "        if cleaned.endswith('.'):\n",
    "            cleaned = cleaned[:-1]\n",
    "        # Eliminar coma final si existe\n",
    "        if cleaned.endswith(','):\n",
    "            cleaned = cleaned[:-1]\n",
    "        return cleaned.strip()\n",
    "    \n",
    "    df['principales_palabras_clave'] = df['principales_palabras_clave'].apply(clean_keywords)\n",
    "    \n",
    "    # Ordenar por cluster y resetear índice\n",
    "    df = df.sort_values('cluster').reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "print(df.head())\n",
    "\n",
    "print(df.principales_palabras_clave.to_list()[0])\n",
    "\n",
    "# Limpiar espacios adicionales en palabras clave\n",
    "df[headers[3]] = df[headers[3]].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "if(df.principales_palabras_clave.to_list()[0] == (\n",
    "        \"maximum power point tracking, \"\n",
    "        \"fuzzy-logic based control, \"\n",
    "        \"photo voltaic (pv), \"\n",
    "        \"photo-voltaic system, \"\n",
    "        \"differential evolution algorithm, \"\n",
    "        \"evolutionary algorithm, \"\n",
    "        \"double-fed induction generator (dfig), \"\n",
    "        \"ant colony optimisation, \"\n",
    "        \"photo voltaic array, \"\n",
    "        \"firefly algorithm, \"\n",
    "        \"partial shade\"\n",
    "    )):\n",
    "    print(\"Respuesta correcta\")\n",
    "else:\n",
    "    print(\"Respuesta incorrecta\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
